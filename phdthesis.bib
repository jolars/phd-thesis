@article{belloni2011,
  title         = {Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic Programming},
  shorttitle    = {Square-Root Lasso},
  author        = {Belloni, Alexandre and Chernozhukov, Victor and Wang, Lie},
  volume        = {98},
  number        = {4},
  pages         = {791--806},
  doi           = {10.1093/biomet/asr043},
  issn          = {0006-3444},
  url           = {https://doi.org/10.1093/biomet/asr043},
  urldate       = {2023-10-18},
  date          = {2011-12},
  journaltitle  = {Biometrika},
  shortjournal  = {Biometrika},
  abstract      = {
    We propose a pivotal method for estimating high-dimensional sparse linear
    regression models, where the overall number of regressors p is large, possibly much
    larger than n, but only s regressors are significant. The method is a modification
    of the lasso, called the square-root lasso. The method is pivotal in that it
    neither relies on the knowledge of the standard deviation \ensuremath{\sigma} nor
    does it need to pre-estimate \ensuremath{\sigma}. Moreover, the method does not
    rely on normality or sub-Gaussianity of noise. It achieves near-oracle performance,
    attaining the convergence rate \ensuremath{\sigma}\{(s/n) log p\}1/2 in the
    prediction norm, and thus matching the performance of the lasso with known
    \ensuremath{\sigma}. These performance results are valid for both Gaussian and
    non-Gaussian errors, under some mild moment restrictions. We formulate the
    square-root lasso as a solution to a convex conic programming problem, which allows
    us to implement the estimator using efficient algorithmic methods, such as
    interior-point and first-order methods.
  }
}

% == BibLateX quality report for belloni2011:
% ? unused Library catalog ("Silverchair")
@article{bertsimas2016,
  title         = {Best Subset Selection via a Modern Optimization Lens},
  author        = {Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
  volume        = {44},
  number        = {2},
  pages         = {813--852},
  doi           = {10.1214/15-AOS1388},
  issn          = {0090-5364},
  url           = {
    https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full
  },
  urldate       = {2024-01-16},
  date          = {2016-04-01},
  journaltitle  = {The Annals of Statistics},
  shortjournal  = {Ann. Statist.},
  langid        = {english}
}

% == BibLateX quality report for bertsimas2016:
% ? unused Library catalog ("DOI.org (Crossref)")
@online{bogdan2013,
  title         = {Statistical Estimation and Testing via the Sorted {{L1}} Norm},
  author        = {
    Bogdan, Ma\l{}gorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false
    and Su, Weijie and Cand\`{e}s, Emmanuel J.
  },
  doi           = {10.48550/arXiv.1310.1969},
  url           = {http://arxiv.org/abs/1310.1969},
  urldate       = {2020-04-16},
  date          = {2013-10-29},
  eprint        = {1310.1969},
  eprinttype    = {arxiv},
  eprintclass   = {math, stat},
  abstract      = {
    We introduce a novel method for sparse regression and variable selection, which is
    inspired by modern ideas in multiple testing. Imagine we have observations from the
    linear model y = X beta + z, then we suggest estimating the regression coefficients
    by means of a new estimator called SLOPE, which is the solution to minimize 0.5
    \vert{}\vert{}y - Xb\textbackslash \vert{}\_2\textasciicircum 2 + lambda\_1
    \vert{}b\vert{}\_(1) + lambda\_2 \vert{}b\vert{}\_(2) + ... + lambda\_p
    \vert{}b\vert{}\_(p); here, lambda\_1 {$>$}= \textbackslash lambda\_2 {\$>\$}= ...
    {\$>\$}= \textbackslash lambda\_p {\$>\$}= 0 and \vert{}b\vert{}\_(1) {\$>\$}=
    \vert{}b\vert{}\_(2) {\$>\$}= ... {\$>\$}= \vert{}b\vert{}\_(p) is the order
    statistic of the magnitudes of b. The regularizer is a sorted L1 norm which
    penalizes the regression coefficients according to their rank: the higher the rank,
    the larger the penalty. This is similar to the famous BHq procedure [Benjamini and
    Hochberg, 1995], which compares the value of a test statistic taken from a family
    to a critical threshold that depends on its rank in the family. SLOPE is a convex
    program and we demonstrate an efficient algorithm for computing the solution. We
    prove that for orthogonal designs with p variables, taking lambda\_i =
    F\textasciicircum\{-1\}(1-q\_i) (F is the cdf of the errors), q\_i = iq/(2p),
    controls the false discovery rate (FDR) for variable selection. When the design
    matrix is nonorthogonal there are inherent limitations on the FDR level and the
    power which can be obtained with model selection methods based on L1-like
    penalties. However, whenever the columns of the design matrix are not strongly
    correlated, we demonstrate empirically that it is possible to select the parameters
    lambda\_i as to obtain FDR control at a reasonable level as long as the number of
    nonzero coefficients is not too large. At the same time, the procedure exhibits
    increased power over the lasso, which treats all coefficients equally. The paper
    illustrates further estimation properties of the new selection rule through
    comprehensive simulation studies.
  },
  pubstate      = {preprint}
}

% == BibLateX quality report for bogdan2013:
% ? unused Number ("arXiv:1310.1969")
@article{bogdan2015,
  title         = {{{SLOPE}} – Adaptive Variable Selection via Convex Optimization},
  author        = {
    Bogdan, Ma\l{}gorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false
    and Sabatti, Chiara and Su, Weijie and Cand\`{e}s, Emmanuel J.
  },
  volume        = {9},
  number        = {3},
  pages         = {1103--1140},
  doi           = {10.1214/15-AOAS842},
  issn          = {1932-6157},
  url           = {https://projecteuclid.org/euclid.aoas/1446488733},
  urldate       = {2018-12-17},
  date          = {2015-09},
  journaltitle  = {The annals of applied statistics},
  shortjournal  = {Ann Appl Stat},
  eprint        = {26709357},
  eprinttype    = {pmid}
}

@online{bogdan2022,
  title         = {Pattern Recovery by {{SLOPE}}},
  author        = {
    Bogdan, Ma\l{}gorzata and Dupuis, Xavier and Graczyk, Piotr and Ko\l{}odziejek,
    Bartosz and Skalski, Tomasz and Tardivel, Patrick and Wilczy\'{n}ski, Maciej
  },
  doi           = {10.48550/arXiv.2203.12086},
  url           = {http://arxiv.org/abs/2203.12086},
  urldate       = {2022-06-03},
  date          = {2022-03-22},
  eprint        = {2203.12086},
  eprinttype    = {arxiv},
  eprintclass   = {math, stat},
  abstract      = {
    LASSO and SLOPE are two popular methods for dimensionality reduction in the
    high-dimensional regression. LASSO can eliminate redundant predictors by setting
    the corresponding regression coefficients to zero, while SLOPE can additionally
    identify clusters of variables with the same absolute values of regression
    coefficients. It is well known that LASSO Irrepresentability Condition is
    sufficient and necessary for the proper estimation of the sign of sufficiently
    large regression coefficients. In this article we formulate an analogous
    Irrepresentability Condition for SLOPE, which is sufficient and necessary for the
    proper identification of the SLOPE pattern, i.e. of the proper sign as well as of
    the proper ranking of the absolute values of individual regression coefficients,
    while proper ranking guarantees a proper clustering. We also provide asymptotic
    results on the strong consistency of pattern recovery by SLOPE when the number of
    columns in the design matrix is fixed while the sample size diverges to infinity.
  },
  pubstate      = {preprint}
}

@article{bondell2008,
  title         = {
    Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of
    Predictors with {{OSCAR}}
  },
  author        = {Bondell, Howard D. and Reich, Brian J.},
  volume        = {64},
  number        = {1},
  pages         = {115--123},
  doi           = {10.1111/j.1541-0420.2007.00843.x},
  issn          = {0006-341X},
  url           = {https://www.jstor.org/stable/25502027},
  urldate       = {2020-01-23},
  date          = {2008-03},
  journaltitle  = {Biometrics},
  eprint        = {25502027},
  eprinttype    = {jstor},
  abstract      = {
    Variable selection can be challenging, particularly in situations with a large
    number of predictors with possibly high correlations, such as gene expression data.
    In this article, a new method called the OSCAR (octagonal shrinkage and clustering
    algorithm for regression) is proposed to simultaneously select variables while
    grouping them into predictive clusters. In addition to improving prediction
    accuracy and interpretation, these resulting groups can then be investigated
    further to discover what contributes to the group having a similar behavior. The
    technique is based on penalized least squares with a geometrically intuitive
    penalty function that shrinks some coefficients to exactly zero. Additionally, this
    penalty yields exact equality of some coefficients, encouraging correlated
    predictors that have a similar effect on the response to form predictive clusters
    represented by a single coefficient. The proposed procedure is shown to compare
    favorably to the existing shrinkage and variable selection techniques in terms of
    both prediction error and model complexity, while yielding the additional grouping
    information.
  }
}

@article{brzyski2018,
  title         = {Group {{SLOPE}} – Adaptive Selection of Groups of Predictors},
  author        = {Brzyski, Damian and Gossmann, Alexej and Su, Weijie and Bogdan, Ma\l{}gorzata},
  pages         = {1--15},
  doi           = {10/gfrd93},
  issn          = {0162-1459},
  url           = {https://amstat.tandfonline.com/doi/full/10.1080/01621459.2017.1411269},
  urldate       = {2018-12-17},
  date          = {2018-01-15},
  journaltitle  = {Journal of the American Statistical Association},
  shortjournal  = {Journal of the American Statistical Association},
  abstract      = {
    Sorted L-One Penalized Estimation (SLOPE; Bogdan et~al. 2013 Bogdan, M., van den
    Berg, E., Su, W., and Cand\`{e}s, E. J. (2013), ``Statistical Estimation and
    Testing via the Ordered \mathscr{l}1 Norm,'' Technical Report 2013-07, Department
    of Statistics. Standford, CA: Stanford University.~[Google Scholar], 2015 Bogdan,
    M., van den Berg, E., Sabatti, C., Su, W., and Cand\`{e}s, E. J. (2015),
    ``SLOPE--Adaptive Variable Selection via Convex Optimization,'' Annals of Applied
    Statistics, 9, 1103–1140.[Crossref], [PubMed], [Web of Science
    \textregistered{}],~,~[Google Scholar]) is a relatively new convex optimization
    procedure, which allows for adaptive selection of regressors under sparse
    high-dimensional designs. Here, we extend the idea of SLOPE to deal with the
    situation when one aims at selecting whole groups of explanatory variables instead
    of single regressors. Such groups can be formed by clustering strongly correlated
    predictors or groups of dummy variables corresponding to different levels of the
    same qualitative predictor. We formulate the respective convex optimization
    problem, group SLOPE (gSLOPE), and propose an efficient algorithm for its solution.
    We also define a notion of the group false discovery rate (gFDR) and provide a
    choice of the sequence of tuning parameters for gSLOPE so that gFDR is provably
    controlled at a prespecified level if the groups of variables are orthogonal to
    each other. Moreover, we prove that the resulting procedure adapts to unknown
    sparsity and is asymptotically minimax with respect to the estimation of the
    proportions of variance of the response variable explained by regressors from
    different groups. We also provide a method for the choice of the regularizing
    sequence when variables in different groups are not orthogonal but statistically
    independent and illustrate its good properties with computer simulations. Finally,
    we illustrate the advantages of gSLOPE in the context of Genome Wide Association
    Studies. R package grpSLOPE with an implementation of our method is available on
    The Comprehensive R Archive Network.
  }
}

% == BibLateX quality report for brzyski2018:
% ? unused Library catalog ("amstat.tandfonline.com (Atypon)")
@article{donoho1994,
  title         = {Ideal Spatial Adaptation by Wavelet Shrinkage},
  author        = {Donoho, David L. and Johnstone, Iain M.},
  volume        = {81},
  number        = {3},
  pages         = {425--455},
  doi           = {10.2307/2337118},
  issn          = {0006-3444},
  url           = {https://www.jstor.org/stable/2337118},
  urldate       = {2023-10-06},
  date          = {1994-08},
  journaltitle  = {Biometrika},
  eprint        = {2337118},
  eprinttype    = {jstor},
  abstract      = {
    With ideal spatial adaptation, an oracle furnishes information about how best to
    adapt a spatially variable estimator, whether piecewise constant, piecewise
    polynomial, variable knot spline, or variable bandwidth kernel, to the unknown
    function. Estimation with the aid of an oracle offers dramatic advantages over
    traditional linear estimation by nonadaptive kernels; however, it is a priori
    unclear whether such performance can be obtained by a procedure relying on the data
    alone. We describe a new principle for spatially-adaptive estimation: selective
    wavelet reconstruction. We show that variable-knot spline fits and
    piecewise-polynomial fits, when equipped with an oracle to select the knots, are
    not dramatically more powerful than selective wavelet reconstruction with an
    oracle. We develop a practical spatially adaptive method, RiskShrink, which works
    by shrinkage of empirical wavelet coefficients. RiskShrink used in connection with
    sample rotation. Inclusion probabilities of any order can be written explicitly in
    closed form. Second-order inclusion probabilities \ensuremath{\pi}ij satisfy the
    condition \$0 {$<\$} \textbackslash pi\_\{ij\} {\$<\$} \textbackslash
    pi\_\{i\}\textbackslash pi\_j\$, which guarantees Yates \& Grundy's variance
    estimator to be unbiased, definable for all samples and always nonnegative for any
    sample size.
  }
}

% == BibLateX quality report for donoho1994:
% Unexpected field 'publisher'
@article{donoho1995,
  title         = {Adapting to Unknown Smoothness via Wavelet Shrinkage},
  author        = {Donoho, David L. and Johnstone, Iain M.},
  publisher     = {American Statistical Association, Taylor \& Francis, Ltd.},
  volume        = {90},
  number        = {432},
  pages         = {1200--1224},
  doi           = {10.2307/2291512},
  issn          = {0162-1459},
  url           = {https://www.jstor.org/stable/2291512},
  urldate       = {2023-11-03},
  date          = {1995},
  journaltitle  = {Journal of the American Statistical Association},
  eprint        = {2291512},
  eprinttype    = {jstor},
  abstract      = {
    We attempt to recover a function of unknown smoothness from noisy sampled data. We
    introduce a procedure, SureShrink, that suppresses noise by thresholding the
    empirical wavelet coefficients. The thresholding is adaptive: A threshold level is
    assigned to each dyadic resolution level by the principle of minimizing the Stein
    unbiased estimate of risk (Sure) for threshold estimates. The computational effort
    of the overall procedure is order N \cdot{} log(N) as a function of the sample size
    N. SureShrink is smoothness adaptive: If the unknown function contains jumps, then
    the reconstruction (essentially) does also; if the unknown function has a smooth
    piece, then the reconstruction is (essentially) as smooth as the mother wavelet
    will allow. The procedure is in a sense optimally smoothness adaptive: It is near
    minimax simultaneously over a whole interval of the Besov scale; the size of this
    interval depends on the choice of mother wavelet. We know from a previous paper by
    the authors that traditional smoothing methods--kernels, splines, and orthogonal
    series estimates--even with optimal choices of the smoothing parameter, would be
    unable to perform in a near-minimax way over many spaces in the Besov scale.
    Examples of SureShrink are given. The advantages of the method are particularly
    evident when the underlying function has jump discontinuities on a smooth
    background.
  }
}

@article{efron2004,
  title         = {Least Angle Regression},
  author        = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain M. and Tibshirani, Robert},
  volume        = {32},
  number        = {2},
  pages         = {407--499},
  doi           = {10.1214/009053604000000067},
  issn          = {0090-5364},
  url           = {https://projecteuclid.org/euclid.aos/1083178935},
  urldate       = {2020-08-20},
  date          = {2004-04},
  journaltitle  = {Annals of Statistics},
  shortjournal  = {Ann. Statist.},
  abstract      = {
    The purpose of model selection algorithms such as All Subsets, Forward Selection
    and Backward Elimination is to choose a linear model on the basis of the same set
    of data to which the model will be applied. Typically we have available a large
    collection of possible covariates from which we hope to select a parsimonious set
    for the efficient prediction of a response variable. Least Angle Regression (LARS),
    a new model selection algorithm, is a useful and less greedy version of traditional
    forward selection methods. Three main properties are derived: (1) A simple
    modification of the LARS algorithm implements the Lasso, an attractive version of
    ordinary least squares that constrains the sum of the absolute regression
    coefficients; the LARS modification calculates all possible Lasso estimates for a
    given problem, using an order of magnitude less computer time than previous
    methods. (2) A different LARS modification efficiently implements Forward Stagewise
    linear regression, another promising new model selection method; this connection
    explains the similar numerical results previously observed for the Lasso and
    Stagewise, and helps us understand the properties of both methods, which are seen
    as constrained versions of the simpler LARS algorithm. (3) A simple approximation
    for the degrees of freedom of a LARS estimate is available, from which we derive a
    Cp estimate of prediction error; this allows a principled choice among the range of
    possible LARS estimates. LARS and its variants are computationally efficient: the
    paper describes a publicly available algorithm that requires only the same order of
    magnitude of computational effort as ordinary least squares applied to the full set
    of covariates.
  },
  langid        = {english}
}

@article{fan2001,
  title         = {Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties},
  author        = {Fan, Jianqing and Li, Runze},
  volume        = {96},
  number        = {456},
  pages         = {1348--1360},
  doi           = {10/fd7bfs},
  issn          = {0162-1459},
  url           = {https://doi.org/10.1198/016214501753382273},
  urldate       = {2018-03-14},
  date          = {2001-12-01},
  journaltitle  = {Journal of the American Statistical Association},
  abstract      = {
    Variable selection is fundamental to high-dimensional statistical modeling,
    including nonparametric regression. Many approaches in use are stepwise selection
    procedures, which can be computationally expensive and ignore stochastic errors in
    the variable selection process. In this article, penalized likelihood approaches
    are proposed to handle these kinds of problems. The proposed methods select
    variables and estimate coefficients simultaneously. Hence they enable us to
    construct confidence intervals for estimated parameters. The proposed approaches
    are distinguished from others in that the penalty functions are symmetric,
    nonconcave on (0, \infty{}), and have singularities at the origin to produce sparse
    solutions. Furthermore, the penalty functions should be bounded by a constant to
    reduce bias and satisfy certain conditions to yield continuous solutions. A new
    algorithm is proposed for optimizing penalized likelihood functions. The proposed
    ideas are widely applicable. They are readily applied to a variety of parametric
    models such as generalized linear models and robust regression models. They can
    also be applied easily to nonparametric modeling by using wavelets and splines.
    Rates of convergence of the proposed penalized likelihood estimators are
    established. Furthermore, with proper choice of regularization parameters, we show
    that the proposed estimators perform as well as the oracle procedure in variable
    selection; namely, they work as well as if the correct submodel were known. Our
    simulation shows that the newly proposed methods compare favorably with other
    variable selection techniques. Furthermore, the standard error formulas are tested
    to be accurate enough for practical applications.
  }
}

% == BibLateX quality report for fan2001:
% ? unused Library catalog ("Taylor and Francis+NEJM")
@unpublished{figueiredo2014,
  title         = {
    Sparse Estimation with Strongly Correlated Variables Using Ordered Weighted {{L1}}
    Regularization
  },
  author        = {Figueiredo, M\'{a}rio A. T. and Nowak, Robert D.},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1409.4005},
  url           = {http://arxiv.org/abs/1409.4005},
  urldate       = {2022-06-03},
  date          = {2014-09-13},
  eprint        = {1409.4005},
  eprinttype    = {arxiv},
  eprintclass   = {stat},
  abstract      = {
    This paper studies ordered weighted L1 (OWL) norm regularization for sparse
    estimation problems with strongly correlated variables. We prove sufficient
    conditions for clustering based on the correlation/colinearity of variables using
    the OWL norm, of which the so-called OSCAR is a particular case. Our results extend
    previous ones for OSCAR in several ways: for the squared error loss, our conditions
    hold for the more general OWL norm and under weaker assumptions; we also establish
    clustering conditions for the absolute error loss, which is, as far as we know, a
    novel result. Furthermore, we characterize the statistical performance of OWL norm
    regularization for generative models in which certain clusters of regression
    variables are strongly (even perfectly) correlated, but variables in different
    clusters are uncorrelated. We show that if the true p-dimensional signal generating
    the data involves only s of the clusters, then O(s log p) samples suffice to
    accurately estimate the signal, regardless of the number of coefficients within the
    clusters. The estimation of s-sparse signals with completely independent variables
    requires just as many measurements. In other words, using the OWL we pay no price
    (in terms of the number of measurements) for the presence of strongly correlated
    variables.
  }
}

% == BibLateX quality report for figueiredo2014:
% Unexpected field 'publisher'
% ? unused Number ("arXiv:1409.4005")
@inproceedings{figueiredo2016,
  title         = {
    Ordered Weighted {{L1}} Regularized Regression with Strongly Correlated Covariates:
    Theoretical Aspects
  },
  shorttitle    = {
    Ordered {{Weighted L1 Regularized Regression}} with {{Strongly Correlated
    Covariates}}
  },
  author        = {Figueiredo, M\'{a}rio A. T. and Nowak, Robert},
  year          = {2016 9-11 May},
  booktitle     = {
    Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}}
    and {{Statistics}}
  },
  location      = {Cadiz, Spain},
  publisher     = {JMLR W\&CP},
  series        = {Proceedings of {{Machine Learning Research}}},
  volume        = {51},
  pages         = {930--938},
  url           = {http://proceedings.mlr.press/v51/figueiredo16.html},
  urldate       = {2019-11-05},
  abstract      = {
    This paper studies the ordered weighted L1 (OWL) family of regularizers for sparse
    linear regression with strongly correlated covariates.  We prove sufficient
    conditions for clustering correlated c...
  },
  eventtitle    = {Artificial {{Intelligence}} and {{Statistics}}},
  langid        = {english}
}

% == BibLateX quality report for figueiredo2016:
% ? unused Library catalog ("proceedings.mlr.press")
@article{friedman2007,
  title         = {Pathwise Coordinate Optimization},
  author        = {Friedman, Jerome and Hastie, Trevor and H\"{o}fling, Holger and Tibshirani, Robert},
  volume        = {1},
  number        = {2},
  pages         = {302--332},
  doi           = {10/d88g8c},
  issn          = {1932-6157},
  url           = {https://projecteuclid.org/euclid.aoas/1196438020},
  urldate       = {2018-03-12},
  date          = {2007-12},
  journaltitle  = {The Annals of Applied Statistics},
  shortjournal  = {Ann. Appl. Stat.},
  abstract      = {
    We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of
    convex optimization problems. An algorithm of this kind has been proposed for the
    L1-penalized regression (lasso) in the literature, but it seems to have been
    largely ignored. Indeed, it seems that coordinate-wise algorithms are not often
    used in convex optimization. We show that this algorithm is very competitive with
    the well-known LARS (or homotopy) procedure in large lasso problems, and that it
    can be applied to related methods such as the garotte and elastic net. It turns out
    that coordinate-wise descent does not work in the ``fused lasso,'' however, so we
    derive a generalized algorithm that yields the solution in much less time that a
    standard convex optimizer. Finally, we generalize the procedure to the
    two-dimensional fused lasso, and demonstrate its performance on some image
    smoothing problems.
  },
  langid        = {english}
}

% == BibLateX quality report for friedman2007:
% ? unused Library catalog ("Project Euclid")
@article{friedman2008,
  title         = {Sparse Inverse Covariance Estimation with the Graphical Lasso},
  author        = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  volume        = {9},
  number        = {3},
  pages         = {432--441},
  doi           = {10.1093/biostatistics/kxm045},
  issn          = {1465-4644},
  url           = {https://doi.org/10.1093/biostatistics/kxm045},
  urldate       = {2024-04-18},
  date          = {2008-07},
  journaltitle  = {Biostatistics},
  shortjournal  = {Biostatistics},
  abstract      = {
    We consider the problem of estimating sparse graphs by a lasso penalty applied to
    the inverse covariance matrix. Using a coordinate descent procedure for the lasso,
    we develop a simple algorithm--the graphical lasso--that is remarkably fast: It
    solves a 1000-node problem (\sim{}500000 parameters) in at most a minute and is
    30–4000 times faster than competing methods. It also provides a conceptual link
    between the exact problem and the approximation suggested by Meinshausen and
    B\"{u}hlmann (2006). We illustrate the method on some cell-signaling data from
    proteomics.
  }
}

% == BibLateX quality report for friedman2008:
% ? unused Library catalog ("Silverchair")
@inproceedings{guyon2004,
  title         = {Result Analysis of the {{NIPS}} 2003 Feature Selection Challenge},
  author        = {Guyon, Isabelle and Gunn, Steve and Ben-Hur, Asa and Dror, Gideon},
  booktitle     = {Advances in Neural Information Processing Systems 17},
  location      = {Vancouver, BC, Canada},
  publisher     = {MIT Press},
  pages         = {545--552},
  isbn          = {978-0-262-19534-8},
  url           = {
    https://papers.nips.cc/paper/2728-result-analysis-of-the-nips-2003-feature-selection-challenge
  },
  urldate       = {2020-03-02},
  editor        = {Saul, Lawrence K. and Weiss, Yair and Bottou, L\'{e}on},
  date          = {2004-12-13/2004-12-18},
  eventtitle    = {Neural {{Information Processing Systems}} 2004}
}

% == BibLateX quality report for guyon2004:
% ? Unsure about the formatting of the booktitle
@book{hastie2009,
  title         = {
    The Elements of Statistical Learning: Data Mining, Inference, and Prediction,
    Second Edition
  },
  shorttitle    = {The {{Elements}} of {{Statistical Learning}}},
  author        = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  location      = {New York},
  publisher     = {Springer-Verlag},
  series        = {Springer {{Series}} in {{Statistics}}},
  isbn          = {978-0-387-84857-0},
  url           = {//www.springer.com/la/book/9780387848570},
  urldate       = {2018-03-12},
  date          = {2009},
  edition       = {2},
  abstract      = {
    During the past decade there has been an explosion in computation and information
    technology. With it have come vast amounts of data in a variety of fields such as
    medicine, biology, finance, and marketing. The challenge of understanding these
    data has led to the development of new tools in the field of statistics, and
    spawned new areas such as data mining, machine learning, and bioinformatics. Many
    of these tools have common underpinnings but are often expressed with different
    terminology. This book describes the important ideas in these areas in a common
    conceptual framework. While the approach is statistical, the emphasis is on
    concepts rather than mathematics. Many examples are given, with a liberal use of
    color graphics. It is a valuable resource for statisticians and anyone interested
    in data mining in science or industry. The book's coverage is broad, from
    supervised learning (prediction) to unsupervised learning. The many topics include
    neural networks, support vector machines, classification trees and boosting---the
    first comprehensive treatment of this topic in any book. This major new edition
    features many topics not covered in the original, including graphical models,
    random forests, ensemble methods, least angle regression and path algorithms for
    the lasso, non-negative matrix factorization, and spectral clustering. There is
    also a chapter on methods for ``wide'' data (p bigger than n), including multiple
    testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome
    Friedman are professors of statistics at Stanford University. They are prominent
    researchers in this area: Hastie and Tibshirani developed generalized additive
    models and wrote a popular book of that title. Hastie co-developed much of the
    statistical modeling software and environment in R/S-PLUS and invented principal
    curves and surfaces. Tibshirani proposed the lasso and is co-author of the very
    successful An Introduction to the Bootstrap. Friedman is the co-inventor of many
    data-mining tools including CART, MARS, projection pursuit and gradient boosting.
  },
  langid        = {english}
}

% == BibLateX quality report for hastie2009:
% ? unused Library catalog ("www.springer.com")
@article{hastie2020,
  title         = {
    Best Subset, Forward Stepwise or Lasso? {{Analysis}} and Recommendations Based on
    Extensive Comparisons
  },
  shorttitle    = {Best {{Subset}}, {{Forward Stepwise}} or {{Lasso}}?},
  author        = {Hastie, Trevor and Tibshirani, Robert and Tibshirani, Ryan},
  volume        = {35},
  number        = {4},
  pages         = {579--592},
  doi           = {10.1214/19-STS733},
  issn          = {0883-4237},
  url           = {https://projecteuclid.org/euclid.ss/1605603631},
  urldate       = {2020-12-02},
  date          = {2020-11},
  journaltitle  = {Statistical Science},
  shortjournal  = {Statist. Sci.},
  abstract      = {
    In exciting recent work, Bertsimas, King and Mazumder (Ann. Statist. 44 (2016)
    813–852) showed that the classical best subset selection problem in regression
    modeling can be formulated as a mixed integer optimization (MIO) problem. Using
    recent advances in MIO algorithms, they demonstrated that best subset selection can
    now be solved at much larger problem sizes than what was thought possible in the
    statistics community. They presented empirical comparisons of best subset with
    other popular variable selection procedures, in particular, the lasso and forward
    stepwise selection. Surprisingly (to us), their simulations suggested that best
    subset consistently outperformed both methods in terms of prediction accuracy.
    Here, we present an expanded set of simulations to shed more light on these
    comparisons. The summary is roughly as follows: \textbullet{}neither best subset
    nor the lasso uniformly dominate the other, with best subset generally performing
    better in very high signal-to-noise (SNR) ratio regimes, and the lasso better in
    low SNR regimes; \textbullet{}for a large proportion of the settings considered,
    best subset and forward stepwise perform similarly, but in certain cases in the
    high SNR regime, best subset performs better; \textbullet{}forward stepwise and
    best subsets tend to yield sparser models (when tuned on a validation set),
    especially in the high SNR regime; \textbullet{}the relaxed lasso (actually, a
    simplified version of the original relaxed estimator defined in Meinshausen
    (Comput. Statist. Data Anal. 52 (2007) 374–393)) is the overall winner, performing
    just about as well as the lasso in low SNR scenarios, and nearly as well as best
    subset in high SNR scenarios.
  },
  langid        = {english}
}

% == BibLateX quality report for hastie2020:
% ? unused Library catalog ("Project Euclid")
@incollection{james1961,
  title         = {Estimation with Quadratic Loss},
  author        = {James, Willard and Stein, Charles},
  booktitle     = {
    Proceedings of the {{Fourth Berkeley Symposium}} on {{Mathematical Statistics}} and
    {{Probability}}, {{Volume}} 1: {{Contributions}} to the {{Theory}} of
    {{Statistics}}
  },
  location      = {Berkeley, USA},
  publisher     = {University of California Press},
  series        = {Berkeley {{Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  volume        = {4.1},
  pages         = {361--380},
  issn          = {0097-0433},
  url           = {
    https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Estimation-with-Quadratic-Loss/bsmsp/1200512173
  },
  urldate       = {2024-04-17},
  date          = {1961},
  abstract      = {Berkeley Symposium on Mathematical Statistics and Probability}
}

% == BibLateX quality report for james1961:
% Unexpected field 'issn'
% Missing required field 'editor'
% ? unused Library catalog ("projecteuclid.org")
@article{kos2020,
  title         = {On the Asymptotic Properties of {{SLOPE}}},
  author        = {Kos, Micha\l{} and Bogdan, Ma\l{}gorzata},
  volume        = {82},
  number        = {2},
  pages         = {499--532},
  doi           = {10.1007/s13171-020-00212-5},
  issn          = {0976-8378},
  url           = {https://doi.org/10.1007/s13171-020-00212-5},
  urldate       = {2022-06-03},
  date          = {2020-08-11},
  journaltitle  = {Sankhya A},
  shortjournal  = {Sankhya A},
  abstract      = {
    Sorted L-One Penalized Estimator (SLOPE) is a relatively new convex optimization
    procedure for selecting predictors in high dimensional regression analyses. SLOPE
    extends LASSO by replacing the L1 penalty norm with a Sorted L1 norm, based on the
    non-increasing sequence of tuning parameters. This allows SLOPE to adapt to unknown
    sparsity and achieve an asymptotic minimax convergency rate under a wide range of
    high dimensional generalized linear models. Additionally, in the case when the
    design matrix is orthogonal, SLOPE with the sequence of tuning parameters
    \ensuremath{\lambda}BH corresponding to the sequence of decaying thresholds for the
    Benjamini-Hochberg multiple testing correction provably controls the False
    Discovery Rate (FDR) in the multiple regression model. In this article we provide
    new asymptotic results on the properties of SLOPE when the elements of the design
    matrix are iid random variables from the Gaussian distribution. Specifically, we
    provide conditions under which the asymptotic FDR of SLOPE based on the sequence
    \ensuremath{\lambda}BH converges to zero and the power converges to 1. We
    illustrate our theoretical asymptotic results with an extensive simulation study.
    We also provide precise formulas describing FDR of SLOPE under different loss
    functions, which sets the stage for future investigation on the model selection
    properties of SLOPE and its extensions.
  },
  langid        = {english}
}

% == BibLateX quality report for kos2020:
% ? unused Library catalog ("Springer Link")
@inproceedings{larsson2020b,
  title         = {The Strong Screening Rule for {{SLOPE}}},
  author        = {Larsson, Johan and Bogdan, Ma\l{}gorzata and Wallin, Jonas},
  booktitle     = {Advances in Neural Information Processing Systems 33},
  location      = {Virtual},
  publisher     = {Curran Associates, Inc.},
  volume        = {33},
  pages         = {14592--14603},
  isbn          = {978-1-71382-954-6},
  url           = {
    https://papers.nips.cc/paper\%5Ffiles/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
  },
  editor        = {
    Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan,
    Maria-Florina and Lin, Hsuan-Tien
  },
  date          = {2020-12-06/2020-12-12},
  abstract      = {
    Extracting relevant features from data sets where the number of observations n is
    much smaller then the number of predictors p is a major challenge in modern
    statistics. Sorted L-One Penalized Estimation (SLOPE)--a generalization of the
    lasso---is a promising method within this setting. Current numerical procedures for
    SLOPE, however, lack the efficiency that respective tools for the lasso enjoy,
    particularly in the context of estimating a complete regularization path. A key
    component in the efficiency of the lasso is predictor screening rules: rules that
    allow  predictors to be discarded before estimating the model. This is the first
    paper to establish such a rule for SLOPE. We develop a screening rule for SLOPE by
    examining its subdifferential and show that this rule is a generalization of the
    strong rule for the lasso. Our rule is heuristic, which means that it may discard
    predictors erroneously. In our paper, however, we show that such situations are
    rare and easily safeguarded against by a simple check of the optimality conditions.
    Our numerical experiments show that the rule performs well in practice, leading to
    improvements by orders of magnitude for data in the \textbackslash (p
    \textbackslash gg n\textbackslash ) domain, as well as incurring no additional
    computational overhead when \$n {$>\$} p\$.
  },
  eventtitle    = {34th Conference on Neural Information Processing Systems ({{NeurIPS}} 2020)},
  langid        = {english}
}

% == BibLateX quality report for larsson2020b:
% ? Unsure about the formatting of the booktitle
@inproceedings{larsson2021,
  title         = {Look-Ahead Screening Rules for the Lasso},
  author        = {Larsson, Johan},
  booktitle     = {22nd {{European}} Young Statisticians Meeting - Proceedings},
  location      = {Athens, Greece},
  publisher     = {{Panteion university of social and political sciences}},
  pages         = {61--65},
  isbn          = {978-960-7943-23-1},
  url           = {https://www.eysm2021.panteion.gr/files/Proceedings\%5FEYSM\%5F2021.pdf},
  editor        = {
    {Andreas Makridis} and {Fotios S. Milienos} and {Panagiotis Papastamoulis} and
    {Christina Parpoula} and {Athanasios Rakitzis}
  },
  date          = {2021-09-06},
  abstract      = {
    The lasso is a popular method to induce shrinkage and sparsity in the solution
    vector (coefficients) of regression problems, particularly when there are many
    predictors relative to the number of observations. Solving the lasso in this
    high-dimensional setting can, however, be computationally demanding. Fortunately,
    this demand can be alleviated via the use of screening rules that discard
    predictors prior to fitting the model, leading to a reduced problem to be solved.
    In this paper, we present a new screening strategy: look-ahead screening. Our
    method uses safe screening rules to find a range of penalty values for which a
    given predictor cannot enter the model, thereby screening predictors along the
    remainder of the path. In experiments we show that these look-ahead screening rules
    outperform the active warm-start version of the Gap Safe rules.
  },
  eventtitle    = {22nd {{European}} Young Statisticians Meeting},
  langid        = {english}
}

% == BibLateX quality report for larsson2021:
% ? Unsure about the formatting of the booktitle
@inproceedings{larsson2022b,
  title         = {The {{Hessian}} Screening Rule},
  author        = {Larsson, Johan and Wallin, Jonas},
  booktitle     = {Advances in Neural Information Processing Systems 35},
  location      = {New Orleans, USA},
  publisher     = {Curran Associates, Inc.},
  volume        = {35},
  pages         = {15823--15835},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://papers.nips.cc/paper\%5Ffiles/paper/2022/hash/65a925049647eab0aa06a9faf1cd470b-Abstract-Conference.html
  },
  editor        = {
    Koyejo, Sanmi and Mohamed, Sidahmed and Agarwal, Alekh and Belgrave, Danielle and
    Cho, Kyunghyun and Oh, Alice
  },
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Predictor screening rules, which discard predictors from the design matrix before
    fitting a model, have had considerable impact on the speed with which
    l1-regularized regression problems, such as the lasso, can be solved. Current
    state-of-the-art screening rules, however, have difficulties in dealing with
    highly-correlated predictors, often becoming too conservative. In this paper, we
    present a new screening rule to deal with this issue: the Hessian Screening Rule.
    The rule uses second-order information from the model to provide more accurate
    screening as well as higher-quality warm starts. The proposed rule outperforms all
    studied alternatives on data sets with high correlation for both l1-regularized
    least-squares (the lasso) and logistic regression. It also performs best overall on
    the real data sets that we examine.
  },
  eventtitle    = {
    36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)
  },
  langid        = {english}
}

% == BibLateX quality report for larsson2022b:
% ? Unsure about the formatting of the booktitle
@inproceedings{larsson2023,
  title         = {Coordinate Descent for {{SLOPE}}},
  author        = {Larsson, Johan and Klopfenstein, Quentin and Massias, Mathurin and Wallin, Jonas},
  booktitle     = {
    Proceedings of the 26th International Conference on Artificial Intelligence and
    Statistics
  },
  location      = {Valencia, Spain},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  volume        = {206},
  pages         = {4802--4821},
  url           = {https://proceedings.mlr.press/v206/larsson23a.html},
  editor        = {
    Ruiz, Francisco and Dy, Jennifer and family=Meent, given=Jan-Willem, prefix=van de,
    useprefix=true
  },
  date          = {2023-04-25/2023-04-27},
  abstract      = {
    The lasso is the most famous sparse regression and feature selection method. One
    reason for its popularity is the speed at which the underlying optimization problem
    can be solved. Sorted L-One Penalized Estimation (SLOPE) is a generalization of the
    lasso with appealing statistical properties. In spite of this, the method has not
    yet reached widespread interest. A major reason for this is that current software
    packages that fit SLOPE rely on algorithms that perform poorly in high dimensions.
    To tackle this issue, we propose a new fast algorithm to solve the SLOPE
    optimization problem, which combines proximal gradient descent and proximal
    coordinate descent steps. We provide new results on the directional derivative of
    the SLOPE penalty and its related SLOPE thresholding operator, as well as provide
    convergence guarantees for our proposed solver. In extensive benchmarks on
    simulated and real data, we demonstrate our method's performance against a long
    list of competing algorithms.
  },
  eventtitle    = {{{AISTATS}} 2023}
}

% == BibLateX quality report for larsson2023:
% ? Unsure about the formatting of the booktitle
@book{lawson1995,
  title         = {Solving Least Squares Problems},
  author        = {Lawson, Charles L. and Hanson, Richard J.},
  location      = {Philadelphia, PA, USA},
  publisher     = {{Society for Industrial and Applied Mathematics}},
  series        = {Classics in {{Applied Mathematics}}},
  doi           = {10.1137/1.9781611971217},
  isbn          = {978-0-89871-356-5},
  url           = {https://epubs.siam.org/doi/book/10.1137/1.9781611971217},
  urldate       = {2024-02-09},
  date          = {1995},
  edition       = {2},
  langid        = {english},
  pagetotal     = {351}
}

% == BibLateX quality report for lawson1995:
% ? unused Library catalog ("epubs.siam.org (Atypon)")
@inproceedings{moreau2022a,
  title         = {Benchopt: Reproducible, Efficient and Collaborative Optimization Benchmarks},
  shorttitle    = {Benchopt},
  author        = {
    Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre and Ablin, Pierre and
    Bannier, Pierre-Antoine and Charlier, Benjamin and Dagr\'{e}ou, Mathieu and
    family=Tour, given=Tom Dupr\'{e}, prefix=la, useprefix=false and Durif, Ghislain
    and Dantas, Cassio F. and Klopfenstein, Quentin and Larsson, Johan and Lai, En and
    Lefort, Tanguy and Mal\'{e}zieux, Benoit and Moufad, Badr and Nguyen, Binh T. and
    Rakotomamonjy, Alain and Ramzi, Zaccharie and Salmon, Joseph and Vaiter, Samuel
  },
  booktitle     = {Advances in Neural Information Processing Systems 35},
  location      = {New Orleans, USA},
  publisher     = {Curran Associates, Inc.},
  volume        = {35},
  pages         = {25404--25421},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://proceedings.neurips.cc/paper\%5Ffiles/paper/2022/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html
  },
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Numerical validation is at the core of machine learning research as it allows to
    assess the actual impact of new methods, and to confirm the agreement between
    theory and practice. Yet, the rapid development of the field poses several
    challenges: researchers are confronted with a profusion of methods to compare,
    limited transparency and consensus on best practices, as well as tedious
    re-implementation work. As a result, validation is often very partial, which can
    lead to wrong conclusions that slow down the progress of research. We propose
    Benchopt, a collaborative framework to automate, reproduce and publish optimization
    benchmarks in machine learning across programming languages and hardware
    architectures. Benchopt simplifies benchmarking for the community by providing an
    off-the-shelf tool for running, sharing and extending experiments. To demonstrate
    its broad usability, we showcase benchmarks on three standard learning tasks:
    \$\textbackslash ell\_2\$-regularized logistic regression, Lasso, and ResNet18
    training for image classification. These benchmarks highlight key practical
    findings that give a more nuanced view of the state-of-the-art for these problems,
    showing that for practical evaluation, the devil is in the details. We hope that
    Benchopt will foster collaborative work in the community hence improving the
    reproducibility of research findings.
  },
  eventtitle    = {36th Conference on Neural Information Processing Systems ({{NeurIPS}} 2022)}
}

% == BibLateX quality report for moreau2022a:
% ? Unsure about the formatting of the booktitle
@article{osborne2000a,
  title         = {A New Approach to Variable Selection in Least Squares Problems},
  author        = {Osborne, Michael R. and Presnell, Brett and Turlach, Berwin A.},
  volume        = {20},
  number        = {3},
  pages         = {389--403},
  doi           = {10.1093/imanum/20.3.389},
  issn          = {1464-3642},
  date          = {2000-07-01},
  journaltitle  = {IMA Journal of Numerical Analysis},
  abstract      = {
    The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a
    technique of variable selection which requires the minimization of a sum of squares
    subject to an l1 bound \ensuremath{\kappa} on the solution. This forces zero
    components in the minimizing solution for small values of \ensuremath{\kappa}. Thus
    this bound can function as a selection parameter. This paper makes two
    contributions to computational problems associated with implementing the Lasso: (1)
    a compact descent method for solving the constrained problem for a particular value
    of \ensuremath{\kappa} is formulated, and (2) a homotopy method, in which the
    constraint bound \ensuremath{\kappa} becomes the homotopy parameter, is developed
    to completely describe the possible selection regimes. Both algorithms have a
    finite termination property. It is suggested that modified Gram-Schmidt
    orthogonalization applied to an augmented design matrix provides an effective basis
    for implementing the algorithms.
  }
}

@article{santosa1986,
  title         = {Linear Inversion of Band-Limited Reflection Seismograms},
  author        = {Santosa, Fadil and Symes, William W.},
  volume        = {7},
  number        = {4},
  pages         = {1307--1330},
  doi           = {10.1137/0907087},
  issn          = {0196-5204},
  url           = {https://epubs.siam.org/doi/10.1137/0907087},
  urldate       = {2023-11-21},
  date          = {1986-10},
  journaltitle  = {SIAM Journal on Scientific and Statistical Computing},
  shortjournal  = {SIAM J. Sci. and Stat. Comput.},
  abstract      = {
    A simple model problem in exploration seismology requires that a depth-varying
    sound velocity distribution be estimated from reflected sound waves. For various
    physical reasons, these reflected signals or echoes have very small Fourier
    coefficients at both very high and very low frequencies. Nonetheless, both
    geophysical practice, based on heuristic considerations, and recent numerical
    evidence indicate that a spectrally complete estimate of the velocity distribution
    is often achievable. We prove a theorem to this effect, showing that ``sufficiently
    rough'' velocity distributions may be recovered from reflected waves under some
    restrictions, independently of the very low- or high-frequency content of the data.
    The main restriction is that the velocity depend only on a single (depth) variable;
    only in this case are sufficiently refined propagation-of-singularity results
    available. The proof is based on a novel variational principle, from which
    numerical algorithms have been derived. These algorithms have been implemented and
    used to estimate velocity distributions from both synthetic and field reflection
    seismograms.
  },
  langid        = {english}
}

@article{schneider2022,
  title         = {The Geometry of Uniqueness, Sparsity and Clustering in Penalized Estimation},
  author        = {Schneider, Ulrike and Tardivel, Patrick},
  volume        = {23},
  number        = {331},
  pages         = {1--36},
  issn          = {1532-4435},
  url           = {https://www.jmlr.org/papers/volume23/21-0420/21-0420.pdf},
  date          = {2022-10-01},
  journaltitle  = {The Journal of Machine Learning Research},
  shortjournal  = {J. Mach. Learn. Res.},
  abstract      = {
    We provide a necessary and sufficient condition for the uniqueness of penalized
    least-squares estimators whose penalty term is given by a norm with a polytope unit
    ball, covering a wide range of methods including SLOPE, PACS, fused, clustered and
    classical LASSO as well as the related method of basis pursuit. We consider a
    strong type of uniqueness that is relevant for statistical problems. The uniqueness
    condition is geometric and involves how the row span of the design matrix
    intersects the faces of the dual norm unit ball, which for SLOPE is given by the
    signed permutahedron. Further considerations based this condition also allow to
    derive results on sparsity and clustering features. In particular, we define the
    notion of a SLOPE pattern to describe both sparsity and clustering properties of
    this method and also provide a geometric characterization of accessible SLOPE
    patterns.
  }
}

% == BibLateX quality report for schneider2022:
% ? unused Library catalog ("ACM Digital Library")
@incollection{stein1956,
  title         = {
    Inadmissibility of the Usual Estimator for the Mean of a Multivariate Normal
    Distribution
  },
  author        = {Stein, Charles},
  booktitle     = {
    Proceedings of the {{Third Berkeley Symposium}} on {{Mathematical Statistics}} and
    {{Probability}}, {{Volume}} 1: {{Contributions}} to the {{Theory}} of
    {{Statistics}}
  },
  location      = {Berkeley, USA},
  publisher     = {University of California Press},
  series        = {Berkeley {{Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  volume        = {3.1},
  issn          = {0097-0433},
  url           = {
    https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Inadmissibility-of-the-Usual-Estimator-for-the-Mean-of-a/bsmsp/1200501656
  },
  urldate       = {2024-04-17},
  editor        = {Neyman, Jerzy},
  date          = {1956}
}

% == BibLateX quality report for stein1956:
% Unexpected field 'issn'
% ? unused Library catalog ("projecteuclid.org")
@article{tibshirani1996,
  title         = {Regression Shrinkage and Selection via the Lasso},
  author        = {Tibshirani, Robert},
  volume        = {58},
  number        = {1},
  pages         = {267--288},
  doi           = {10.1111/j.2517-6161.1996.tb02080.x},
  issn          = {0035-9246},
  url           = {http://www.jstor.org/stable/2346178},
  urldate       = {2018-03-12},
  date          = {1996},
  journaltitle  = {Journal of the Royal Statistical Society: Series B},
  eprint        = {2346178},
  eprinttype    = {jstor},
  abstract      = {
    We propose a new method for estimation in linear models. The `lasso' minimizes the
    residual sum of squares subject to the sum of the absolute value of the
    coefficients being less than a constant. Because of the nature of this constraint
    it tends to produce some coefficients that are exactly 0 and hence gives
    interpretable models. Our simulation studies suggest that the lasso enjoys some of
    the favourable properties of both subset selection and ridge regression. It
    produces interpretable models like subset selection and exhibits the stability of
    ridge regression. There is also an interesting relationship with recent work in
    adaptive function estimation by Donoho and Johnstone. The lasso idea is quite
    general and can be applied in a variety of statistical models: extensions to
    generalized regression models and tree-based models are briefly described.
  },
  langid        = {english}
}

@article{tibshirani2005,
  title         = {Sparsity and Smoothness via the Fused Lasso},
  author        = {
    Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and
    Knight, Keith
  },
  volume        = {67},
  number        = {1},
  pages         = {91--108},
  doi           = {10.1111/j.1467-9868.2005.00490.x},
  issn          = {1467-9868},
  url           = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00490.x},
  urldate       = {2021-04-26},
  date          = {2005-02},
  journaltitle  = {Journal of the Royal Statistical Society: Series B},
  abstract      = {
    Summary. The lasso penalizes a least squares regression by the sum of the absolute
    values (L1-norm) of the coefficients. The form of this penalty encourages sparse
    solutions (with many coefficients equal to 0). We propose the `fused lasso', a
    generalization that is designed for problems with features that can be ordered in
    some meaningful way. The fused lasso penalizes the L1-norm of both the coefficients
    and their successive differences. Thus it encourages sparsity of the coefficients
    and also sparsity of their differences--i.e. local constancy of the coefficient
    profile. The fused lasso is especially useful when the number of features p is much
    greater than N, the sample size. The technique is also extended to the `hinge' loss
    function that underlies the support vector classifier. We illustrate the methods on
    examples from protein mass spectroscopy and gene expression data.
  },
  langid        = {english}
}

% == BibLateX quality report for tibshirani2005:
% ? unused Library catalog ("Wiley Online Library")
@article{yuan2005,
  title         = {Model Selection and Estimation in Regression with Grouped Variables},
  author        = {Yuan, Ming and Lin, Yi},
  volume        = {68},
  number        = {1},
  pages         = {49--67},
  doi           = {10.1111/j.1467-9868.2005.00532.x},
  issn          = {1467-9868},
  url           = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
  urldate       = {2024-04-18},
  date          = {2005-12-21},
  journaltitle  = {Journal of the Royal Statistical Society: Series B},
  abstract      = {
    Summary. We consider the problem of selecting grouped variables (factors) for
    accurate prediction in regression. Such a problem arises naturally in many
    practical situations with the multifactor analysis-of-variance problem as the most
    important and well-known example. Instead of selecting factors by stepwise backward
    elimination, we focus on the accuracy of estimation and consider extensions of the
    lasso, the LARS algorithm and the non-negative garrotte for factor selection. The
    lasso, the LARS algorithm and the non-negative garrotte are recently proposed
    regression methods that can be used to select individual variables. We study and
    propose efficient algorithms for the extensions of these methods for factor
    selection and show that these extensions give superior performance to the
    traditional stepwise backward elimination method in factor selection problems. We
    study the similarities and the differences between these methods. Simulations and
    real examples are used to illustrate the methods.
  },
  langid        = {english}
}

% == BibLateX quality report for yuan2005:
% ? unused extra: _eprint ("https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x")
% ? unused Library catalog ("Wiley Online Library")
@article{zeng2014,
  title         = {Decreasing Weighted Sorted L1 Regularization},
  author        = {Zeng, Xiangrong and Figueiredo, M\'{a}rio A. T.},
  volume        = {21},
  number        = {10},
  pages         = {1240--1244},
  doi           = {10.1109/LSP.2014.2331977},
  issn          = {1070-9908, 1558-2361},
  date          = {2014-10},
  journaltitle  = {IEEE Signal Processing Letters},
  abstract      = {
    We consider a new family of regularizers, termed weighted sorted \mathscr{l}1 norms
    (WSL1), which generalizes the recently introduced octagonal shrinkage and
    clustering algorithm for regression (OSCAR) and also contains the \mathscr{l}1 and
    \mathscr{l}\infty{} norms as particular instances. We focus on a special case of
    the WSL1, the decreasing WSL1 (DWSL1), where the elements of the argument vector
    are sorted in non-increasing order and the weights are also non-increasing. In this
    letter, after showing that the DWSL1 is indeed a norm, we derive two key tools for
    its use as a regularizer: the dual norm and the Moreau proximity operator.
  }
}

% == BibLateX quality report for zeng2014:
% 'issn': not a valid ISSN
% ? unused Library catalog ("IEEE Xplore")
@inproceedings{zeng2014a,
  title         = {
    The Atomic Norm Formulation of {{OSCAR}} Regularization with Application to the
    {{Frank}}–{{Wolfe}} Algorithm
  },
  author        = {Zeng, Xiangrong and Figueiredo, M\'{a}rio A. T.},
  booktitle     = {{{EUSIPCO}} 2014},
  location      = {Lisbon, Portugal},
  publisher     = {IEEE},
  pages         = {780--784},
  isbn          = {978-0-9928626-1-9},
  url           = {https://ieeexplore.ieee.org/document/6952255},
  date          = {2014-09-01/2014-09-05},
  abstract      = {
    This paper proposes atomic norm formulation of octagonal shrinkage and clustering
    algorithm for regression (OSCAR) regularization. The OSCAR regularizer can be
    reformulated using a decreasing weighted sorted l1 (DWSL1) norm (which is shown to
    be convex). We also show how, by exploiting an atomic norm formulation, the Ivanov
    regularization scheme involving the OSCAR regularizer can be handled using the
    Frank-Wolfe (also known as conditional gradient) method.
  },
  eventtitle    = {{{EUSIPCO}} 2014}
}

% == BibLateX quality report for zeng2014a:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("IEEE Xplore")
@unpublished{zeng2015,
  title         = {The Ordered Weighted L1 Norm: Atomic Formulation, Projections, and Algorithms},
  shorttitle    = {The {{Ordered Weighted L1 Norm}}},
  author        = {Zeng, Xiangrong and Figueiredo, M\'{a}rio A. T.},
  url           = {http://arxiv.org/abs/1409.4271},
  urldate       = {2019-11-22},
  date          = {2015-04-10},
  eprint        = {1409.4271},
  eprinttype    = {arxiv},
  eprintclass   = {cs, math},
  abstract      = {
    The ordered weighted \$\textbackslash ell\_1\$ norm (OWL) was recently proposed,
    with two different motivations: its good statistical properties as a sparsity
    promoting regularizer; the fact that it generalizes the so-called \{\textbackslash
    it octagonal shrinkage and clustering algorithm for regression\} (OSCAR), which has
    the ability to cluster/group regression variables that are highly correlated. This
    paper contains several contributions to the study and application of OWL
    regularization: the derivation of the atomic formulation of the OWL norm; the
    derivation of the dual of the OWL norm, based on its atomic formulation; a new and
    simpler derivation of the proximity operator of the OWL norm; an efficient scheme
    to compute the Euclidean projection onto an OWL ball; the instantiation of the
    conditional gradient (CG, also known as Frank-Wolfe) algorithm for linear
    regression problems under OWL regularization; the instantiation of accelerated
    projected gradient algorithms for the same class of problems. Finally, a set of
    experiments give evidence that accelerated projected gradient algorithms are
    considerably faster than CG, for the class of problems considered.
  }
}

@article{zhang2010,
  title         = {Nearly Unbiased Variable Selection under Minimax Concave Penalty},
  author        = {Zhang, Cun-Hui},
  volume        = {38},
  number        = {2},
  pages         = {894--942},
  doi           = {10/bp22zz},
  issn          = {0090-5364},
  url           = {https://projecteuclid.org/euclid.aos/1266586618},
  urldate       = {2018-03-14},
  date          = {2010-04},
  journaltitle  = {The Annals of Statistics},
  shortjournal  = {Ann. Statist.},
  abstract      = {
    We propose MC+, a fast, continuous, nearly unbiased and accurate method of
    penalized variable selection in high-dimensional linear regression. The LASSO is
    fast and continuous, but biased. The bias of the LASSO may prevent consistent
    variable selection. Subset selection is unbiased but computationally costly. The
    MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear
    unbiased selection (PLUS) algorithm. The MCP provides the convexity of the
    penalized loss in sparse regions to the greatest extent given certain thresholds
    for variable selection and unbiasedness. The PLUS computes multiple exact local
    minimizers of a possibly nonconvex penalized loss function in a certain main branch
    of the graph of critical points of the penalized loss. Its output is a continuous
    piecewise linear path encompassing from the origin for infinite penalty to a least
    squares solution for zero penalty. We prove that at a universal penalty level, the
    MC+ has high probability of matching the signs of the unknowns, and thus correct
    selection, without assuming the strong irrepresentable condition required by the
    LASSO. This selection consistency applies to the case of p\gg{}n, and is proved to
    hold for exactly the MC+ solution among possibly many local minimizers. We prove
    that the MC+ attains certain minimax convergence rates in probability for the
    estimation of regression coefficients in \mathscr{l}r balls. We use the SURE method
    to derive degrees of freedom and Cp-type risk estimates for general penalized LSE,
    including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the
    estimated degrees of freedom, we propose an estimator of the noise level for proper
    choice of the penalty level. For full rank designs and general sub-quadratic
    penalties, we provide necessary and sufficient conditions for the continuity of the
    penalized LSE. Simulation results overwhelmingly support our claim of superior
    variable selection properties and demonstrate the computational efficiency of the
    proposed method.
  },
  langid        = {english},
  mrnumber      = {MR2604701},
  zmnumber      = {1183.62120}
}

% == BibLateX quality report for zhang2010:
% Unexpected field 'mrnumber'
% Unexpected field 'zmnumber'
% ? unused Library catalog ("Project Euclid")
@article{zou2005,
  title         = {Regularization and Variable Selection via the {{Elastic Net}}},
  author        = {Zou, Hui and Hastie, Trevor},
  volume        = {67},
  number        = {2},
  pages         = {301--320},
  issn          = {1369-7412},
  url           = {www.jstor.org/stable/3647580},
  urldate       = {2018-03-12},
  date          = {2005},
  journaltitle  = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  abstract      = {
    We propose the elastic net, a new regularization and variable selection method.
    Real world data and a simulation study show that the elastic net often outperforms
    the lasso, while enjoying a similar sparsity of representation. In addition, the
    elastic net encourages a grouping effect, where strongly correlated predictors tend
    to be in or out of the model together. The elastic net is particularly useful when
    the number of predictors (p) is much bigger than the number of observations (n). By
    contrast, the lasso is not a very satisfactory variable selection method in the p
    \gg{} n case. An algorithm called LARS-EN is proposed for computing elastic net
    regularization paths efficiently, much like algorithm LARS does for the lasso.
  }
}

% == BibLateX quality report for zou2005:
% ? Possibly abbreviated journal title Journal of the Royal Statistical Society. Series B (Statistical Methodology)
% ? unused Library catalog ("JSTOR")
@article{zou2006,
  title         = {The Adaptive Lasso and Its Oracle Properties},
  author        = {Zou, Hui},
  publisher     = {Taylor \& Francis},
  volume        = {101},
  number        = {476},
  pages         = {1418--1429},
  doi           = {10.1198/016214506000000735},
  issn          = {0162-1459},
  date          = {2006-12-01},
  journaltitle  = {Journal of the American Statistical Association},
  abstract      = {
    The lasso is a popular technique for simultaneous estimation and variable
    selection. Lasso variable selection has been shown to be consistent under certain
    conditions. In this work we derive a necessary condition for the lasso variable
    selection to be consistent. Consequently, there exist certain scenarios where the
    lasso is inconsistent for variable selection. We then propose a new version of the
    lasso, called the adaptive lasso, where adaptive weights are used for penalizing
    different coefficients in the \mathscr{l}1 penalty. We show that the adaptive lasso
    enjoys the oracle properties; namely, it performs as well as if the true underlying
    model were given in advance. Similar to the lasso, the adaptive lasso is shown to
    be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same
    efficient algorithm for solving the lasso. We also discuss the extension of the
    adaptive lasso in generalized linear models and show that the oracle properties
    still hold under mild regularity conditions. As a byproduct of our theory, the
    nonnegative garotte is shown to be consistent for variable selection.
  }
}

% == BibLateX quality report for zou2006:
% Unexpected field 'publisher'
% ? unused extra: _eprint ("https://doi.org/10.1198/016214506000000735")
% ? unused Library catalog ("Taylor and Francis+NEJM")
@inproceedings{mairal2012,
  title         = {Complexity Analysis of the Lasso Regularization Path},
  author        = {Mairal, Julien and Yu, Bin},
  booktitle     = {Proceedings of the 29th {{International Conference}} on {{Machine Learning}}},
  location      = {Edinburgh, United Kingdom},
  pages         = {1835--1842},
  url           = {https://icml.cc/2012/papers/202.pdf},
  date          = {2012-06},
  abstract      = {
    The regularization path of the Lasso can be shown to be piecewise linear, making it
    possible to ``follow'' and explicitly compute the entire path. We analyze in this
    paper this popular strategy, and prove that its worst case complexity is
    exponential in the number of variables. We then oppose this pessimistic result to
    an (optimistic) approximate analysis: We show \surd{}that an approximate path with
    at most O(1/ \ensuremath{\epsilon}) linear segments can always be obtained, where
    every point on the path is guaranteed to be optimal up to a relative
    \ensuremath{\epsilon}-duality gap. We complete our theoretical analysis with a
    practical algorithm to compute these approximate paths.
  },
  eventtitle    = {International {{Conference}} on {{Machine Learning}} 2012},
  langid        = {english}
}

% == BibLateX quality report for mairal2012:
% ? unused Library catalog ("Zotero")
@article{friedman2010,
  title         = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author        = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  volume        = {33},
  number        = {1},
  pages         = {1--22},
  doi           = {10.18637/jss.v033.i01},
  url           = {http://www.jstatsoft.org/v33/i01/},
  date          = {2010-01},
  journaltitle  = {Journal of Statistical Software}
}

@online{dupuis2023,
  title         = {The Solution Path of {{SLOPE}}},
  author        = {Dupuis, Xavier and Tardivel, Patrick J C},
  number        = {hal-04100441v2},
  url           = {https://hal.science/hal-04100441v2},
  urldate       = {2024-01-17},
  date          = {2023-10-28},
  eprint        = {hal-04100441v2},
  eprinttype    = {HAL},
  abstract      = {
    The SLOPE estimator has the particularity of having null components (sparsity) and
    components that are equal in absolute value (clustering). The number of clusters
    depends on the regularization parameter of the estimator. This parameter can be
    chosen as a trade-off between interpretability (with a small number of clusters)
    and accuracy (with a small mean squared error or a small prediction error). Finding
    such a compromise requires to compute the solution path, that is the function
    mapping the regularization parameter to the estimator. We provide in this article
    an algorithm to compute the solution path of SLOPE.
  },
  langid        = {english},
  pubstate      = {preprint}
}

% == BibLateX quality report for dupuis2023:
% Unexpected field 'number'
% ? unused Library catalog ("HAL Archives Ouvertes")
@online{nomura2020,
  title         = {An Exact Solution Path Algorithm for {{SLOPE}} and Quasi-Spherical {{OSCAR}}},
  author        = {Nomura, Shunichi},
  doi           = {10.48550/arXiv.2010.15511},
  url           = {http://arxiv.org/abs/2010.15511},
  urldate       = {2021-05-27},
  date          = {2020-10-29},
  eprint        = {2010.15511},
  eprinttype    = {arxiv},
  abstract      = {
    Sorted \$L\_1\$ penalization estimator (SLOPE) is a regularization technique for
    sorted absolute coefficients in high-dimensional regression. By arbitrarily setting
    its regularization weights \$\textbackslash lambda\$ under the monotonicity
    constraint, SLOPE can have various feature selection and clustering properties. On
    weight tuning, the selected features and their clusters are very sensitive to the
    tuning parameters. Moreover, the exhaustive tracking of their changes is difficult
    using grid search methods. This study presents a solution path algorithm that
    provides the complete and exact path of solutions for SLOPE in fine-tuning
    regularization weights. A simple optimality condition for SLOPE is derived and used
    to specify the next splitting point of the solution path. This study also proposes
    a new design of a regularization sequence \$\textbackslash lambda\$ for feature
    clustering, which is called the quasi-spherical and octagonal shrinkage and
    clustering algorithm for regression (QS-OSCAR). QS-OSCAR is designed with a contour
    surface of the regularization terms most similar to a sphere. Among several
    regularization sequence designs, sparsity and clustering performance are compared
    through simulation studies. The numerical observations show that QS-OSCAR performs
    feature clustering more efficiently than other designs.
  },
  pubstate      = {preprint}
}

% == BibLateX quality report for nomura2020:
% ? unused Number ("arXiv:2010.15511")

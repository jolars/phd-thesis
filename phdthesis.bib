@article{bogdan2015,
  title         = {{{SLOPE}} â€“ Adaptive Variable Selection via Convex Optimization},
  author        = {
    Bogdan, Ma\l{}gorzata and family=Berg, given=Ewout, prefix=van den, useprefix=false
    and Sabatti, Chiara and Su, Weijie and Cand\`{e}s, Emmanuel J.
  },
  volume        = {9},
  number        = {3},
  pages         = {1103--1140},
  doi           = {10.1214/15-AOAS842},
  issn          = {1932-6157},
  url           = {https://projecteuclid.org/euclid.aoas/1446488733},
  urldate       = {2018-12-17},
  date          = {2015-09},
  journaltitle  = {The annals of applied statistics},
  shortjournal  = {Ann Appl Stat},
  eprint        = {26709357},
  eprinttype    = {pmid}
}

@article{friedman2007,
  title         = {Pathwise Coordinate Optimization},
  author        = {Friedman, Jerome and Hastie, Trevor and H\"{o}fling, Holger and Tibshirani, Robert},
  volume        = {1},
  number        = {2},
  pages         = {302--332},
  doi           = {10/d88g8c},
  issn          = {1932-6157},
  url           = {https://projecteuclid.org/euclid.aoas/1196438020},
  urldate       = {2018-03-12},
  date          = {2007-12},
  journaltitle  = {The Annals of Applied Statistics},
  shortjournal  = {Ann. Appl. Stat.},
  abstract      = {
    We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of
    convex optimization problems. An algorithm of this kind has been proposed for the
    L1-penalized regression (lasso) in the literature, but it seems to have been
    largely ignored. Indeed, it seems that coordinate-wise algorithms are not often
    used in convex optimization. We show that this algorithm is very competitive with
    the well-known LARS (or homotopy) procedure in large lasso problems, and that it
    can be applied to related methods such as the garotte and elastic net. It turns out
    that coordinate-wise descent does not work in the ``fused lasso,'' however, so we
    derive a generalized algorithm that yields the solution in much less time that a
    standard convex optimizer. Finally, we generalize the procedure to the
    two-dimensional fused lasso, and demonstrate its performance on some image
    smoothing problems.
  },
  langid        = {english}
}

@inproceedings{larsson2020b,
  title         = {The Strong Screening Rule for {{SLOPE}}},
  author        = {Larsson, Johan and Bogdan, Ma\l{}gorzata and Wallin, Jonas},
  booktitle     = {Advances in {{Neural Information Processing Systems}} 33},
  location      = {{Virtual}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {33},
  pages         = {14592--14603},
  isbn          = {978-1-71382-954-6},
  url           = {
    https://papers.nips.cc/paper\_files/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
  },
  editor        = {
    Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan,
    Maria-Florina and Lin, Hsuan-Tien
  },
  date          = {2020-12-06/0012},
  abstract      = {
    Extracting relevant features from data sets where the number of observations n is
    much smaller then the number of predictors p is a major challenge in modern
    statistics. Sorted L-One Penalized Estimation (SLOPE)--a generalization of the
    lasso---is a promising method within this setting. Current numerical procedures for
    SLOPE, however, lack the efficiency that respective tools for the lasso enjoy,
    particularly in the context of estimating a complete regularization path. A key
    component in the efficiency of the lasso is predictor screening rules: rules that
    allow  predictors to be discarded before estimating the model. This is the first
    paper to establish such a rule for SLOPE. We develop a screening rule for SLOPE by
    examining its subdifferential and show that this rule is a generalization of the
    strong rule for the lasso. Our rule is heuristic, which means that it may discard
    predictors erroneously. In our paper, however, we show that such situations are
    rare and easily safeguarded against by a simple check of the optimality conditions.
    Our numerical experiments show that the rule performs well in practice, leading to
    improvements by orders of magnitude for data in the \textbackslash (p
    \textbackslash gg n\textbackslash ) domain, as well as incurring no additional
    computational overhead when \$n {$>\$} p\$.
  },
  eventtitle    = {
    34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)
  },
  langid        = {english}
}

@inproceedings{larsson2021,
  title         = {Look-Ahead Screening Rules for the Lasso},
  author        = {Larsson, Johan},
  booktitle     = {22nd {{European Young Statisticians Meeting}} - {{Proceedings}}},
  location      = {{Athens, Greece}},
  publisher     = {{Panteion University of Social and Political Sciences}},
  pages         = {61--65},
  isbn          = {978-960-7943-23-1},
  url           = {https://www.eysm2021.panteion.gr/files/Proceedings\_EYSM\_2021.pdf},
  editor        = {
    {Andreas Makridis} and {Fotios S. Milienos} and {Panagiotis Papastamoulis} and
    {Christina Parpoula} and {Athanasios Rakitzis}
  },
  date          = {2021-09-06},
  abstract      = {
    The lasso is a popular method to induce shrinkage and sparsity in the solution
    vector (coefficients) of regression problems, particularly when there are many
    predictors relative to the number of observations. Solving the lasso in this
    high-dimensional setting can, however, be computationally demanding. Fortunately,
    this demand can be alleviated via the use of screening rules that discard
    predictors prior to fitting the model, leading to a reduced problem to be solved.
    In this paper, we present a new screening strategy: look-ahead screening. Our
    method uses safe screening rules to find a range of penalty values for which a
    given predictor cannot enter the model, thereby screening predictors along the
    remainder of the path. In experiments we show that these look-ahead screening rules
    outperform the active warm-start version of the Gap Safe rules.
  },
  eventtitle    = {22nd {{European Young Statisticians Meeting}}},
  langid        = {english},
  keywords      = {mine}
}

@inproceedings{larsson2022b,
  title         = {The {{Hessian}} Screening Rule},
  author        = {Larsson, Johan and Wallin, Jonas},
  booktitle     = {Advances in {{Neural Information Processing Systems}} 35},
  location      = {{New Orleans, USA}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {35},
  pages         = {15823--15835},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://papers.nips.cc/paper\_files/paper/2022/hash/65a925049647eab0aa06a9faf1cd470b-Abstract-Conference.html
  },
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Predictor screening rules, which discard predictors from the design matrix before
    fitting a model, have had considerable impact on the speed with which
    l1-regularized regression problems, such as the lasso, can be solved. Current
    state-of-the-art screening rules, however, have difficulties in dealing with
    highly-correlated predictors, often becoming too conservative. In this paper, we
    present a new screening rule to deal with this issue: the Hessian Screening Rule.
    The rule uses second-order information from the model to provide more accurate
    screening as well as higher-quality warm starts. The proposed rule outperforms all
    studied alternatives on data sets with high correlation for both l1-regularized
    least-squares (the lasso) and logistic regression. It also performs best overall on
    the real data sets that we examine.
  },
  eventtitle    = {
    36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)
  },
  langid        = {english},
  keywords      = {mine}
}

@inproceedings{larsson2023,
  title         = {Coordinate Descent for {{SLOPE}}},
  author        = {Larsson, Johan and Klopfenstein, Quentin and Massias, Mathurin and Wallin, Jonas},
  booktitle     = {
    Proceedings of the 26th International Conference on Artificial Intelligence and
    Statistics
  },
  location      = {{Valencia, Spain}},
  publisher     = {{PMLR}},
  series        = {Proceedings of Machine Learning Research},
  volume        = {206},
  pages         = {4802--4821},
  url           = {https://proceedings.mlr.press/v206/larsson23a.html},
  editor        = {
    Ruiz, Francisco and Dy, Jennifer and family=Meent, given=Jan-Willem, prefix=van de,
    useprefix=true
  },
  date          = {2023-04-25/2023-04-27},
  abstract      = {
    The lasso is the most famous sparse regression and feature selection method. One
    reason for its popularity is the speed at which the underlying optimization problem
    can be solved. Sorted L-One Penalized Estimation (SLOPE) is a generalization of the
    lasso with appealing statistical properties. In spite of this, the method has not
    yet reached widespread interest. A major reason for this is that current software
    packages that fit SLOPE rely on algorithms that perform poorly in high dimensions.
    To tackle this issue, we propose a new fast algorithm to solve the SLOPE
    optimization problem, which combines proximal gradient descent and proximal
    coordinate descent steps. We provide new results on the directional derivative of
    the SLOPE penalty and its related SLOPE thresholding operator, as well as provide
    convergence guarantees for our proposed solver. In extensive benchmarks on
    simulated and real data, we demonstrate our method's performance against a long
    list of competing algorithms.
  },
  eventtitle    = {{{AISTATS}} 2023},
  keywords      = {mine}
}

@inproceedings{moreau2022a,
  title         = {Benchopt: Reproducible, Efficient and Collaborative Optimization Benchmarks},
  shorttitle    = {Benchopt},
  author        = {
    Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre and Ablin, Pierre and
    Bannier, Pierre-Antoine and Charlier, Benjamin and Dagr\'{e}ou, Mathieu and
    family=Tour, given=Tom Dupr\'{e}, prefix=la, useprefix=false and Durif, Ghislain
    and Dantas, Cassio F. and Klopfenstein, Quentin and Larsson, Johan and Lai, En and
    Lefort, Tanguy and Mal\'{e}zieux, Benoit and Moufad, Badr and Nguyen, Binh T. and
    Rakotomamonjy, Alain and Ramzi, Zaccharie and Salmon, Joseph and Vaiter, Samuel
  },
  booktitle     = {Advances in {{Neural Information Processing Systems}} 35},
  location      = {{New Orleans, USA}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {35},
  pages         = {25404--25421},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://proceedings.neurips.cc/paper\_files/paper/2022/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html
  },
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Numerical validation is at the core of machine learning research as it allows to
    assess the actual impact of new methods, and to confirm the agreement between
    theory and practice. Yet, the rapid development of the field poses several
    challenges: researchers are confronted with a profusion of methods to compare,
    limited transparency and consensus on best practices, as well as tedious
    re-implementation work. As a result, validation is often very partial, which can
    lead to wrong conclusions that slow down the progress of research. We propose
    Benchopt, a collaborative framework to automate, reproduce and publish optimization
    benchmarks in machine learning across programming languages and hardware
    architectures. Benchopt simplifies benchmarking for the community by providing an
    off-the-shelf tool for running, sharing and extending experiments. To demonstrate
    its broad usability, we showcase benchmarks on three standard learning tasks:
    \$\textbackslash ell\_2\$-regularized logistic regression, Lasso, and ResNet18
    training for image classification. These benchmarks highlight key practical
    findings that give a more nuanced view of the state-of-the-art for these problems,
    showing that for practical evaluation, the devil is in the details. We hope that
    Benchopt will foster collaborative work in the community hence improving the
    reproducibility of research findings.
  },
  eventtitle    = {
    36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)
  },
  keyword       = {mine}
}

\documentclass{book}

\input{tex/preamble.tex}
\input{tex/macros.tex}

% Re-usable information (title page, datasheet)
\newcommand{\myName}{Johan Larsson}
\newcommand{\myYear}{2024}
\newcommand{\myMainTitle}{Optimization and Algorithms in Sparse Regression}
\newcommand{\mySubTitle}{Screening Rules, Coordinate Descent, and Normalization}

% Compact combination of combining title and subtitle, used in datasheet and as
% chapter title for the main part.
\newcommand{\myTitle}{\myMainTitle: \mySubTitle}

\newcommand{\myFaculty}{Lund University School of Economics and Management}
\newcommand{\myDepartment}{Department of Statistics}
\newcommand{\myAddress}{Box 7080\\SE--220 07 Lund\\Sweden} % Three lines maximum!

\newcommand{\myDegree}{Thesis for the degree of Doctor of Philosophy}
\newcommand{\myAdvisors}{Jonas Wallin and Małgorzata Bogdan}
\newcommand{\myOpponent}{Professor Mário A. T. Figueiredo (Instituto Superior Técnico, Lisbon, Portugal)}

\newcommand{\myDefenceAnnouncement}{%
  By due permission of the Department of Statistics,\\
  School of Economics and Management, Lund University, Sweden.\\
  To be defended in Karlssonsalen, Holger Crafoord Centre, Lund\\
  on June~28, \myYear\xspace, at 10:15.
}
\newcommand{\myCoverFront}{%
  \noindent {\bf Cover illustration front:} The elastic net path for a dataset of diabetes patients
}
\newcommand{\myCoverBack}{%
  {\bf Cover illustration back:} A picture of me.}
\newcommand{\myFundingInformation}{%
  {\bf Funding information:} This thesis received no external funding.}

% \newcommand{\mySeries}{\ISSN: $<$ISSN number$>$}

\newcommand{\myISBNprint}{978-91-8104-076-0}
\newcommand{\myISBNpdf}{978-91-8104-077-7}
\newcommand{\myFormSignDate}{2024-05-05}
% TODO: fix form sign date

% total page number is computed automatically:
\newcommand{\myPages}{\pageref*{LastPages}}

\newcommand{\myFormDefenceDate}{2024-06-28}
\newcommand{\myFormKeywords}{%
  high-dimensional statistics, sparse regression, lasso, ridge regression, SLOPE, optimization, screening rules, normalization}

% Write a short english abstract of the thesis here, goes into the data sheet page 4. 
\newcommand{\myAbstract}{%
  Datasets are growing in size and complexity, especially with respect to the number of features of the problems that we study, which now often number in the millions. This has lead to a surge in interest for sparse regression models, which help make sense of these datasets by modeling them efficiently whilst still retaining a notion of explainability. Because these datasets are so large, however, they have prompted a need for effective methods with which to apply them---in this thesis, we present several contributions to this area of research.

  In papers \I--\III, we focus on screening rules for the lasso and sorted \(\ell_1\) penalized regression (SLOPE)---two sparse regression methods. Screening rules are algorithms that discard a portion of the features in the model before solving it, which means that we effectively get to tackle a smaller problem than the original ones, yet still recover the same solutions. For the lasso, there has been a large body of work on screening rules since they were first introduced in 2010. In the case of SLOPE, however, there did not exist any screening rule until our work in paper~\I, in which we introduce the first such rule: the strong screening rule for SLOPE.

  In paper \II, we continue our work on screening rules by introducing look-ahead screening rules for the lasso, which enable screening of features for a stretch of the lasso path, rather than just for the following step. In essence, this allows us save computation time by screening features only when it is necessary. In paper \III, we then tackle the case of using screening rules with highly correlated features, which is a setting in which previous screening rules have struggled. We propose the Hessian screening rule, which uses second-order information about the problem in order to provide less conservative screening along the lasso path. In empirical studies we show that our screening rule leads to large improvements in performance.

  In paper~\IV, we introduce benchopt: a framework for benchmarking optimization methods in a transparent, reproducible, and collaborative manner. The current field of research in optimization is overflowing with new algorithms, each time proclaimed by its authors to improve upon its predecessors. It is easy to find benchmarks that directly contradict one another, which often stems for varied use of parameters, different software implementations, and hardware setups. Benchopt makes it easy to construct benchmarks that transparently and objectively compare these methods to one another.

  One particularly effective optimization method for the lasso is coordinate descent. Unfortunately, we cannot directly use coordinate descent for SLOPE since the problem is not separable. In paper~\V, however, we present a hybrid method which circumvents this issue by incorporating proximal gradient descent steps to tackle the separability issue, whilst still enjoying the effectiveness of coordinate descent.

  In the final paper, paper~\VI, we study the use of normalization for the lasso and ridge regression when the data is made up of binary features. Normalization is necessary in regularized regression to put features on the same scale, but its effects are generally not well-understood. In our paper we show that the solutions in the lasso and ridge regression depend strongly on the class balance of the binary features and that this effect depends on the type of normalization used.
}%

% --- Shortcut thesis commands for paper references and titles ---
% Included papers - only need to type this once
\newcommand{\PaperIauthor}{Johan Larsson, Małgorzata Bogdan, and Jonas Wallin}
\newcommand{\PaperIIauthor}{Johan Larsson}
\newcommand{\PaperIIIauthor}{Johan Larsson and Jonas Wallin}
\newcommand{\PaperIVauthor}{Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier, Benjamin Charlier, Mathieu Dagréou, Tom Dupré la Tour, Ghislain Durif, Cassio F. Dantas, Quentin Klopfenstein, Johan Larsson, En Lai, Tanguy Lefort, Benoit Malézieux, Badr Moufad, Binh T. Nguyen, Alain Rakotomamonjy, Zaccharie Ramzi, Joseph Salmon, and Samuel Vaiter}
\newcommand{\PaperVauthor}{Johan Larsson, Quentin Klopfenstein, Mathurin Massias, and Jonas Wallin}
\newcommand{\PaperVIauthor}{Johan Larsson and Jonas Wallin}
% \newcommand{\PaperIref}{Journal for Superheroes, pp. 1--10}
% \newcommand{\PaperIIref}{Journal for Bankers, pp. 2--30}
% \newcommand{\PaperIIIref}{Journal for Superheroes, pp. 1--10}
% \newcommand{\PaperIVref}{Journal for Superheroes, pp. 1--10}
% \newcommand{\PaperVref}{Journal for Superheroes, pp. 1--10}
% \newcommand{\PaperVIref}{Journal for Superheroes, pp. 1--10}

\newcommand{\PaperItitle}{The Strong Screening Rule for SLOPE}
\newcommand{\PaperIItitle}{Look-Ahead Screening Rules for the Lasso}
\newcommand{\PaperIIItitle}{The Hessian Screening Rule}
\newcommand{\PaperIVtitle}{Benchopt: Reproducible, Efficient and Collaborative Optimization Benchmarks}
\newcommand{\PaperVtitle}{Coordinate Descent for SLOPE}
\newcommand{\PaperVItitle}{The Lasso and Ridge Regression Yield Biased Estimates of Imbalanced Binary Features}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\scshape\romannumeral #1}
\makeatother

\newcommand{\PaperSeparator}[1]{%
  \begin{tikzpicture}[remember picture, overlay]
    \node[
      fill=black,
      text=white,
      font=\bfseries\fontsize{50}{55}\selectfont,
      anchor=east,
      minimum height=1.5cm,
      minimum width=2.6cm,
      text width=2.0cm,
      align=left
      ] at ($(current page.north east) + (0,-{#1*0.15}\paperheight + 0.75cm)$)  {\rom{#1}};
  \end{tikzpicture}
}

% Not included papers, in case you want to list them
%\newcommand{\PaperNotIncIauthor}{C. Coauthor, \textbf{X. Someone}}
%\newcommand{\PaperNotIncIref}{Journal for \ldots}
%\newcommand{\PaperNotIncItitle}{Boring matter}
% --- END reusable thesis commands ---

\begin{document}

%%%%%%%%%%%%% First pages with thesis title etc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First six pages are contained in a separate file. Normally you should not
% need to edit it.

\pagestyle{empty}

\includepdf[]{covers/front.pdf}
\cleardoublepage

\input{tex/frontmatter.tex}

% Quote page

\newpage
\thispagestyle{empty} % No page number on quote page

\setlength\epigraphrule{0pt}
\setlength\epigraphwidth{.5\textwidth}

\null
\vspace{20ex}


\epigraph{\itshape
  There’s a point when you go with what you’ve got. Or you don’t go.
}{
  ---Joan Didion
}

\cleardoublepage

\pagestyle{fancy}

% Table of Contents
\setcounter{page}{1} % Page Roman 1 of the frontmatter
\setcounter{tocdepth}{1}

{\hypersetup{hidelinks}\tableofcontents}
\addtocontents{toc}{\protect\thispagestyle{empty}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% List of publications %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Have a long list and want to start on the left page? Here, have a special
% chapter heading for that!
%\addcontentsline{toc}{part}{Part 1: Summary}
%\renewcommand{\chapterheadstartvskip}{}
%{\let\cleardoublepage\relax \let\chapterheadstartvskip\nop 

\newpage

\chap{Acknowledgements}

% \epigraph{\itshape
%   And all the fat, skinny people
%
%   And all the tall, short people
%
%   And all the nobody people
%
%   And all the somebody people
%
%   I never thought I'd need so many people
% }{
%   ---David Bowie
% }

\epigraph{\itshape
  % We've got five years, stuck on my eyes
  %
  % Five years, what a surprise

  We've got five years, my brain hurts a lot

  Five years, that's all we've got
}{
  ---David Bowie
}

% \epigraph{\itshape
%   The best thing about being a statistician is you get to play in everybody else's backyard.
% }{
%   ---John Tukey
% }

\noindent It is a perpetual source of amazement to me how anyone could let me loose for five years to do research about more or less whatever I like---and pay me for it. The flip side of this, unfortunately, is that it has only served to exacerbate the impostor syndrome that has kept me company throughout the years.

That's not to say that doing a PhD is \emph{easy}.

It was not particularly easy.

But it \emph{was} made so much easier by all the help I received along the way.

First of all, I want to thank my supervisor Jonas Wallin who has been encouraging and supportive throughout all of my PhD. He has always found time to help out, even when swamped with work of his own, and always shown interest in my ideas even when they were terrible, yet never hesitated to tell me so. I am grateful to him for looking beyond my unorthodox background when accepting me into the program and sorry for my knack for scheduling vacation time during important submission deadlines.

On the same note, I also want to thank my co-supervisor Małgorzata Bogdan for her support throughout the PhD and her patience for my ignorant questions about high-dimensional statistics and SLOPE, which she in fact introduced me to. I especially want to thank her and her husband Krzystof for hosting me in Wrocław and hope I get the pleasure to come back some time in the future.

The Department of Statistics has (with the exception of that time when the school tried to shut us down) been a great place to work. I have appreciated the seemingly universal open-doors policy that its members have subscribed to and the flat organization that have made it easy to participate in the department's activities and voice one's opinions. I believe the latter fact is testament to the leadership style that the department heads during my time, the two Krzystofs: Nowicki and Podgórski, have had.

I want to thank the administrative staff for keeping the ship afloat and for always being helpful and friendly. And I also want to thank the other members of the department for creating a stimulating research environment, friendly lunch-room enviroment, and for the many interesting discussions and fun kick-offs that we have had over the years.

Doing a PhD is solitary, sometimes lonely, work, yet would be much lonelier without the other PhD students at the department: Henrik, Yvette, Ivan, Hanqing, and Mattias. They have been supportive and helpful, reading manuscripts and giving feedback on my work. But mostly I  want to thank them for their friendship and for the many lunches, fikas, hikes, and after-work drinks we have shared. I hope we get to become colleagues again in the future.

Above and beyond all, however, I want to thank my family. My parents, Kerstin and Torbjörn, for all the love and support throughout my life irrespective of the choices I have made. And Henrik and Emma for being there, always. Tove, for following me through thick and thin, being supportive and understanding, and for enduring the bouts of frustration and stress that come with the territory of fast-approaching deadlines---including the one that inches ever closer as I write this text. I am sorry for leaving you hanging as I've gone to conferences half-way across the world, but grateful that I could. I am blessed to have you by my side.
And finally, Hugo, for being the light of my life.

\bigskip

\begin{flushright}
  \noindent \emph{Johan Larsson} \\
  \noindent \emph{May 5, 2024}\\
\end{flushright}

% \newpage

\chap{Abstract}

\myAbstract

\chap{Popular Science Summary}

In this thesis we study the field of big data, where the sheer volume and complexity of information can be overwhelming. We focus on sparse regression models, a type of statistical model that helps make sense of large datasets by simplifying them in the form of a sparse  representation.

The first three papers of the thesis concentrate on screening rules for two types of sparse regression methods: the lasso and sorted \(\ell_1\) penalized regression (SLOPE). Screening rules are like filters that remove some of the less important features of the data before your computer is tasked with processing it. This makes the problem smaller and easier to solve, yet still provides the same results. We introduce the first-ever screening rule for SLOPE and develop look-ahead screening rules for the lasso, which save additional computation time when you want to solve several lasso models at once. Finally, we also tackle the challenge of using screening rules when data features are highly correlated, proposing a new rule that improves performance in this situation.

The fourth paper introduces benchopt, a tool for comparing different optimization methods. With so many new algorithms being developed, it is hard to know which one is best. Benchopt provides a transparent and objective way to compare these methods.

The fifth paper presents a new optimization method for the lasso, a popular sparse regression model. In the method, we combine two existing methods to overcome a limitation of the lasso, making it more effective.

The final paper explores the impact of normalization---a process that attempts to put varying features of the data on the same scale---on the lasso when dealing with binary features. We find that the balance of the binary features (the ratio between ones and zeros) and the type of normalization used can significantly affect the results.

\chap{List of Publications}\label{sec:paperlist}

This thesis is based on the following publications.

% \I	  & {\bf \PaperItitle}\\[2mm]
% 	  & \PaperIauthor\\
%           & \PaperIref\\[6mm] 

\begin{description}[leftmargin=!,labelwidth=0.7cm]
  % \item[\I] \fullcite{larsson2020b}
  \item[\I] \papertitle{\PaperItitle}\papersep
        Johan Larsson, Małgorzata Bogdan, and Jonas Wallin (Dec. 6--12, 2020). In:
        \publication{Advances in Neural Information Processing Systems 33.} 34th Conference on Neural Information Processing Systems (NeurIPS 2020). Ed. by Hugo Larochelle et al. Vol. 33. Virtual: Curran Associates, Inc., pp. 14592--14603. \isbn{978-1-71382-954-6}
        % \item[\II] \fullcite{larsson2021}
  \item[\II] \papertitle{\PaperIItitle}\papersep
        Johan Larsson (Sept. 6, 2021). In:
        \publication{22nd European Young Statisticians Meeting -- Proceedings.} 22nd European Young
        Statisticians Meeting. Ed. by Andreas Makridis, Fotios S. Milienos, Panagiotis
        Papastamoulis, Christina Parpoula, and Athanasios Rakitzis. Athens, Greece:
        Panteion University of Social and Political Sciences, pp. 61–65. \isbn{978-960-
          7943-23-1}
        % \item[\III] \fullcite{larsson2022b}
  \item[\III]\papertitle{\PaperIIItitle}\papersep
        Johan Larsson and Jonas Wallin (Nov. 28--Dec. 9, 2022). In: \publication{Advances in Neural Information Processing Systems 35.} 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Ed. by Sanmi Koyejo, Sidahmed Mohamed, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, and Alice Oh. Vol. 35. New Orleans, USA: Curran Associates, Inc., pp. 15823–15835. \isbn{978-1-71387-108-8}
        % \item[\IV] \fullcite{moreau2022a}
  \item[\IV] \papertitle{\PaperIVtitle}\papersep
        Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier, Benjamin Charlier, Mathieu Dagréou, Tom Dupré la Tour, Ghislain Durif, Cassio F. Dantas, Quentin Klopfenstein, Johan Larsson, En Lai, Tanguy Lefort, Benoit Malézieux, Badr Moufad, Binh T. Nguyen, Alain Rakotomamonjy, Zaccharie Ramzi, Joseph Salmon, and Samuel Vaiter (Nov. 28--Dec. 9, 2022). In: \publication{Advances in Neural Information Processing Systems 35.} 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Ed. by Sanmi Koyejo, Sidahmed Mohamed, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, and Alice Oh. Vol. 35. New Orleans, USA: Curran Associates, Inc., pp. 25404–25421. \isbn{978-1-71387-108-8}
        % \item[\V] \fullcite{larsson2023}
  \item[\V] \papertitle{\PaperVtitle}\papersep
        Johan Larsson, Quentin Klopfenstein, Mathurin Massias, and Jonas Wallin (Apr. 25--27, 2023). In: \publication{Proceedings of the 26th International Conference on Artificial Intelligence and Statistics.} AISTATS 2023. Ed. by Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent. Vol. 206.
        Proceedings of Machine Learning Research. Valencia, Spain: PMLR, pp. 4802--4821
  \item[\VI] \papertitle{\PaperVItitle}\papersep
        Johan Larsson and Jonas Wallin (May 5, 2024). \publication{Preprint.}
\end{description}

\noindent {\small All papers are reproduced with permission of their respective publishers.}

% Page numbers arabic 
\mainmatter
% Reset table counters to not count the publications table
\setcounter{table}{0}
% Reset page counters, this is where it all starts!
\setcounter{page}{1}

\input{tex/introduction.tex}

\printbibliography[heading=subbibliography]

\chap{Papers}
\thispagestyle{empty}

\cleardoublepage

\thispagestyle{empty}
\phantomsection
\PaperSeparator{1}
\addcontentsline{toc}{section}{\protect\numberline{\I} {\PaperItitle}}
\markright{\I}
\cleardoublepage

% \begin{tikzpicture}[remember picture, overlay]
%   \node[fill=black, text=white, font=\bfseries\fontsize{50}{55}\selectfont, anchor=east, minimum height=1.5cm, minimum width=2.5cm, text width=1.5cm, align=left] at ($(current page.north east) + (0,-0.2\paperheight)$) {\I};
% \end{tikzpicture}

\includepdf[
  pages=-,
  width=0.9\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=107 50 506 743,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper1-main.pdf}
\includepdf[
  pages=-,
  width=0.9\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=107 50 506 743,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper1-erratum.pdf}
\includepdf[
  pages=-,
  width=0.92\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=133 110 478 665,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper1-supplement.pdf}

\cleardoublepage

\thispagestyle{empty}
\phantomsection
\PaperSeparator{2}
\addcontentsline{toc}{section}{\protect\numberline{\II} {\PaperIItitle}}
\markright{\II}
\cleardoublepage

\includepdf[
  pages=-,
  width=\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=107 125 468 667,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper2.pdf}

\cleardoublepage

\thispagestyle{empty}
\phantomsection
\PaperSeparator{3}
\addcontentsline{toc}{section}{\protect\numberline{\III} {\PaperIIItitle}}
\markright{\III}
\cleardoublepage

\includepdf[
  pages=-,
  width=0.9\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=107 50 506 743,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper3.pdf}

\cleardoublepage

\thispagestyle{empty}
\phantomsection
\PaperSeparator{4}
\addcontentsline{toc}{section}{\protect\numberline{\IV} {\PaperIVtitle}}
\markright{\IV}
\cleardoublepage

\includepdf[
  pages=-,
  width=0.9\textwidth,
  offset=-14.56723pt 0pt,
  % frame=true,
  clip,
  viewport=107 50 506 743,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper4.pdf}

\cleardoublepage

\thispagestyle{empty}
\phantomsection
\PaperSeparator{5}
\addcontentsline{toc}{section}{\protect\numberline{\V} {\PaperVtitle}}
\markright{\V}
\cleardoublepage

\includepdf[
  pages=-,
  width=\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=62 49 551 721,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper5.pdf}

\cleardoublepage

\thispagestyle{empty}
\phantomsection
\PaperSeparator{6}
\addcontentsline{toc}{section}{\protect\numberline{\VI} {\PaperVItitle}}
\markright{\VI}
\cleardoublepage

\includepdf[
  pages=-,
  width=\textwidth,
  offset=-14.56723pt 0pt,
  clip,
  viewport=71 55 541 712,
  pagecommand={\thispagestyle{fancy}},
]{papers/paper6.pdf}

\includepdf[]{covers/back.pdf}

\end{document}
